{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1bd830",
   "metadata": {},
   "source": [
    "# Career Recommender System - Data Preprocessing\n",
    "\n",
    "This notebook preprocesses user profiles and job catalog data to prepare for the career recommendation system. \n",
    "\n",
    "**Workflow:**\n",
    "1. Install dependencies and setup environment\n",
    "2. Data loading and exploration\n",
    "3. Text preprocessing and feature engineering\n",
    "4. Embedding generation using sentence-transformers\n",
    "5. Vector database setup with FAISS\n",
    "6. Export processed data for training\n",
    "\n",
    "## üìã Prerequisites for Colab/Kaggle\n",
    "- Upload the data files (`sample_users.csv`, `sample_jobs.csv`) to your environment\n",
    "- Or use the sample data generation code provided below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68526f",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "**‚ö†Ô∏è Run this cell first in Colab/Kaggle environments:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90859f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Colab/Kaggle environments\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn plotly\n",
    "!pip install sentence-transformers transformers torch\n",
    "!pip install faiss-cpu xgboost\n",
    "!pip install python-jobspy>=1.1.79 datasets>=2.14.0 serpapi>=1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6e477",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb663fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import faiss\n",
    "\n",
    "# Text processing and embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Environment setup\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36adfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set up Hugging Face authentication\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    from huggingface_hub import login\n",
    "    login(hf_token)\n",
    "    print(\"Hugging Face authentication successful!\")\n",
    "else:\n",
    "    print(\"No HF_TOKEN found - using public models only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9096123",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c82dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets - try real data first, fallback to sample data\n",
    "print(\"üîç Looking for datasets...\")\n",
    "\n",
    "# Try to import our data utilities\n",
    "try:\n",
    "    from src.data_utils import load_real_or_sample_data\n",
    "    REAL_DATA_AVAILABLE = True\n",
    "    print(\"‚úÖ Real data utilities available\")\n",
    "except ImportError:\n",
    "    REAL_DATA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Real data utilities not available, using sample data\")\n",
    "\n",
    "# Load job data\n",
    "if REAL_DATA_AVAILABLE:\n",
    "    # Load real job dataset (lukebarousse/data_jobs) or fallback to sample\n",
    "    jobs_df = load_real_or_sample_data(max_samples=10000, prefer_real=True)\n",
    "    print(f\"‚úÖ Loaded jobs dataset: {len(jobs_df)} jobs\")\n",
    "else:\n",
    "    # Fallback to original sample data loading\n",
    "    jobs_df = pd.read_csv(DATA_DIR / \"sample_jobs.csv\")\n",
    "    print(f\"‚úÖ Loaded sample jobs: {len(jobs_df)} jobs\")\n",
    "\n",
    "# Load user data (always from sample for now)\n",
    "users_df = pd.read_csv(DATA_DIR / \"sample_users.csv\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Users dataset shape: {users_df.shape}\")\n",
    "print(f\"Jobs dataset shape: {jobs_df.shape}\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\n=== USERS DATASET ===\")\n",
    "print(users_df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(users_df.head())\n",
    "\n",
    "print(\"\\n=== JOBS DATASET ===\")\n",
    "print(jobs_df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(jobs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47910aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# GPA distribution\n",
    "axes[0, 0].hist(users_df['gpa'], bins=20, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('GPA Distribution')\n",
    "axes[0, 0].set_xlabel('GPA')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Education level distribution\n",
    "education_counts = users_df['education_level'].value_counts()\n",
    "axes[0, 1].pie(education_counts.values, labels=education_counts.index, autopct='%1.1f%%')\n",
    "axes[0, 1].set_title('Education Level Distribution')\n",
    "\n",
    "# Experience years distribution\n",
    "axes[1, 0].hist(users_df['experience_years'], bins=15, alpha=0.7, color='lightgreen')\n",
    "axes[1, 0].set_title('Experience Years Distribution')\n",
    "axes[1, 0].set_xlabel('Years')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Industry distribution in jobs\n",
    "industry_counts = jobs_df['industry'].value_counts().head(10)\n",
    "axes[1, 1].barh(range(len(industry_counts)), industry_counts.values)\n",
    "axes[1, 1].set_yticks(range(len(industry_counts)))\n",
    "axes[1, 1].set_yticklabels(industry_counts.index)\n",
    "axes[1, 1].set_title('Top Industries in Job Catalog')\n",
    "axes[1, 1].set_xlabel('Number of Jobs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9db80",
   "metadata": {},
   "source": [
    "## 3. User Profile Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_fields(text):\n",
    "    \"\"\"Clean and normalize text fields\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Convert to lowercase, remove extra spaces\n",
    "    text = str(text).lower().strip()\n",
    "    # Remove special characters but keep commas for skill/interest separation\n",
    "    text = re.sub(r'[^\\w\\s,]', '', text)\n",
    "    return text\n",
    "\n",
    "def parse_skills_interests(text):\n",
    "    \"\"\"Parse comma-separated skills/interests into list\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "    items = [item.strip() for item in str(text).split(',')]\n",
    "    return [item for item in items if item]  # Remove empty strings\n",
    "\n",
    "# Clean user profiles\n",
    "users_clean = users_df.copy()\n",
    "\n",
    "# Clean text fields\n",
    "users_clean['interests_clean'] = users_clean['interests'].apply(clean_text_fields)\n",
    "users_clean['skills_clean'] = users_clean['skills'].apply(clean_text_fields)\n",
    "users_clean['field_of_study_clean'] = users_clean['field_of_study'].apply(clean_text_fields)\n",
    "\n",
    "# Parse into lists\n",
    "users_clean['interests_list'] = users_clean['interests_clean'].apply(parse_skills_interests)\n",
    "users_clean['skills_list'] = users_clean['skills_clean'].apply(parse_skills_interests)\n",
    "\n",
    "# Create combined text for embeddings\n",
    "users_clean['profile_text'] = (\n",
    "    users_clean['field_of_study_clean'] + ' ' + \n",
    "    users_clean['interests_clean'] + ' ' + \n",
    "    users_clean['skills_clean']\n",
    ").str.strip()\n",
    "\n",
    "# Normalize GPA to 0-1 scale\n",
    "users_clean['gpa_normalized'] = users_clean['gpa'] / 4.0\n",
    "\n",
    "# Encode education levels\n",
    "education_levels = ['High School', 'Associate', 'Bachelor', 'Master', 'PhD']\n",
    "education_mapping = {level: i for i, level in enumerate(education_levels)}\n",
    "users_clean['education_level_encoded'] = users_clean['education_level'].map(education_mapping)\n",
    "\n",
    "print(\"User profiles preprocessed!\")\n",
    "print(f\"Sample profile text: {users_clean['profile_text'].iloc[0]}\")\n",
    "print(f\"Education encoding: {dict(list(education_mapping.items())[:3])}\")\n",
    "print(users_clean[['user_id', 'gpa_normalized', 'education_level_encoded', 'profile_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca3b02f",
   "metadata": {},
   "source": [
    "## 4. Job Catalog Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_experience_years(exp_text):\n",
    "    \"\"\"Extract minimum experience years from requirement text\"\"\"\n",
    "    if pd.isna(exp_text):\n",
    "        return 0\n",
    "    \n",
    "    # Look for patterns like \"2-4 years\", \"3+ years\", \"1-3 years\"\n",
    "    numbers = re.findall(r'(\\d+)', str(exp_text))\n",
    "    if numbers:\n",
    "        return int(numbers[0])  # Take first number as minimum\n",
    "    return 0\n",
    "\n",
    "def parse_salary_range(salary_text):\n",
    "    \"\"\"Parse salary range and return min, max, and average\"\"\"\n",
    "    if pd.isna(salary_text):\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    numbers = re.findall(r'(\\d+)', str(salary_text))\n",
    "    if len(numbers) >= 2:\n",
    "        min_sal, max_sal = int(numbers[0]), int(numbers[1])\n",
    "        avg_sal = (min_sal + max_sal) / 2\n",
    "        return min_sal, max_sal, avg_sal\n",
    "    elif len(numbers) == 1:\n",
    "        sal = int(numbers[0])\n",
    "        return sal, sal, sal\n",
    "    return 0, 0, 0\n",
    "\n",
    "# Clean job data\n",
    "jobs_clean = jobs_df.copy()\n",
    "\n",
    "# Clean text fields\n",
    "jobs_clean['description_clean'] = jobs_clean['description'].apply(clean_text_fields)\n",
    "jobs_clean['required_skills_clean'] = jobs_clean['required_skills'].apply(clean_text_fields)\n",
    "jobs_clean['job_title_clean'] = jobs_clean['job_title'].apply(clean_text_fields)\n",
    "\n",
    "# Parse skills\n",
    "jobs_clean['required_skills_list'] = jobs_clean['required_skills_clean'].apply(parse_skills_interests)\n",
    "\n",
    "# Extract experience requirements\n",
    "jobs_clean['min_experience_years'] = jobs_clean['experience_requirement'].apply(extract_experience_years)\n",
    "\n",
    "# Parse salary information\n",
    "salary_info = jobs_clean['salary_range'].apply(parse_salary_range)\n",
    "jobs_clean['salary_min'] = [s[0] for s in salary_info]\n",
    "jobs_clean['salary_max'] = [s[1] for s in salary_info]\n",
    "jobs_clean['salary_avg'] = [s[2] for s in salary_info]\n",
    "\n",
    "# Encode education requirements\n",
    "jobs_clean['education_requirement_encoded'] = jobs_clean['education_requirement'].map(education_mapping)\n",
    "jobs_clean['education_requirement_encoded'] = jobs_clean['education_requirement_encoded'].fillna(0)\n",
    "\n",
    "# Create job text for embeddings\n",
    "jobs_clean['job_text'] = (\n",
    "    jobs_clean['job_title_clean'] + ' ' + \n",
    "    jobs_clean['description_clean'] + ' ' + \n",
    "    jobs_clean['required_skills_clean']\n",
    ").str.strip()\n",
    "\n",
    "print(\"Job catalog preprocessed!\")\n",
    "print(f\"Sample job text: {jobs_clean['job_text'].iloc[0][:100]}...\")\n",
    "print(f\"Experience distribution: {jobs_clean['min_experience_years'].value_counts().sort_index()}\")\n",
    "print(jobs_clean[['job_id', 'salary_avg', 'min_experience_years', 'education_requirement_encoded']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1bc52",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a434ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_skill_overlap(user_skills, job_skills):\n",
    "    \"\"\"Calculate overlap between user skills and job requirements\"\"\"\n",
    "    if not user_skills or not job_skills:\n",
    "        return 0.0\n",
    "    \n",
    "    user_set = set(user_skills)\n",
    "    job_set = set(job_skills)\n",
    "    intersection = len(user_set.intersection(job_set))\n",
    "    union = len(user_set.union(job_set))\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def create_user_job_features(user_row, job_row):\n",
    "    \"\"\"Create features for a user-job pair\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Basic compatibility features\n",
    "    features['gpa_normalized'] = user_row['gpa_normalized']\n",
    "    features['experience_years'] = user_row['experience_years']\n",
    "    features['education_level_match'] = 1 if user_row['education_level_encoded'] >= job_row['education_requirement_encoded'] else 0\n",
    "    features['experience_match'] = 1 if user_row['experience_years'] >= job_row['min_experience_years'] else 0\n",
    "    \n",
    "    # Skill overlap\n",
    "    features['skill_overlap'] = calculate_skill_overlap(\n",
    "        user_row['skills_list'], \n",
    "        job_row['required_skills_list']\n",
    "    )\n",
    "    \n",
    "    # Education over-qualification (might be negative for some positions)\n",
    "    features['education_overqualified'] = max(0, user_row['education_level_encoded'] - job_row['education_requirement_encoded'])\n",
    "    \n",
    "    # Experience over-qualification\n",
    "    features['experience_overqualified'] = max(0, user_row['experience_years'] - job_row['min_experience_years'])\n",
    "    \n",
    "    # Salary features (normalized)\n",
    "    features['salary_avg_normalized'] = job_row['salary_avg'] / 150000.0  # Normalize by reasonable max\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Generate sample training data (user-job pairs with features)\n",
    "print(\"Generating sample user-job interaction features...\")\n",
    "sample_features = []\n",
    "\n",
    "# Create positive examples (good matches) and negative examples\n",
    "for _, user in users_clean.head(5).iterrows():  # Limited sample for demo\n",
    "    for _, job in jobs_clean.head(10).iterrows():\n",
    "        features = create_user_job_features(user, job)\n",
    "        features['user_id'] = user['user_id']\n",
    "        features['job_id'] = job['job_id']\n",
    "        \n",
    "        # Simple heuristic for creating labels (in real scenario, use actual user feedback)\n",
    "        label = 1 if (features['education_level_match'] and \n",
    "                     features['skill_overlap'] > 0.1 and \n",
    "                     features['gpa_normalized'] > 0.7) else 0\n",
    "        features['label'] = label\n",
    "        \n",
    "        sample_features.append(features)\n",
    "\n",
    "features_df = pd.DataFrame(sample_features)\n",
    "print(f\"Generated {len(features_df)} user-job feature vectors\")\n",
    "print(f\"Positive examples: {features_df['label'].sum()}\")\n",
    "print(f\"Feature columns: {list(features_df.columns)}\")\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319bc73c",
   "metadata": {},
   "source": [
    "## 6. Text Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f743a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer model\n",
    "print(\"Loading sentence transformer model...\")\n",
    "model_name = \"all-MiniLM-L6-v2\"  # Fast and efficient model\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Generate embeddings for user profiles\n",
    "print(\"Generating user profile embeddings...\")\n",
    "user_texts = users_clean['profile_text'].tolist()\n",
    "user_embeddings = embedding_model.encode(\n",
    "    user_texts, \n",
    "    show_progress_bar=True, \n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"Generated {user_embeddings.shape[0]} user embeddings with dimension {user_embeddings.shape[1]}\")\n",
    "\n",
    "# Generate embeddings for job descriptions\n",
    "print(\"Generating job description embeddings...\")\n",
    "job_texts = jobs_clean['job_text'].tolist()\n",
    "job_embeddings = embedding_model.encode(\n",
    "    job_texts, \n",
    "    show_progress_bar=True, \n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"Generated {job_embeddings.shape[0]} job embeddings with dimension {job_embeddings.shape[1]}\")\n",
    "\n",
    "# Add embeddings to dataframes\n",
    "users_clean['embedding'] = list(user_embeddings)\n",
    "jobs_clean['embedding'] = list(job_embeddings)\n",
    "\n",
    "print(\"Embeddings generated and stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b522f42f",
   "metadata": {},
   "source": [
    "## 7. Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f530267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index for job embeddings\n",
    "embedding_dim = job_embeddings.shape[1]\n",
    "\n",
    "# Use L2 distance (cosine similarity can also be used)\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add job embeddings to index\n",
    "index.add(job_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} job embeddings\")\n",
    "print(f\"Index dimension: {embedding_dim}\")\n",
    "\n",
    "# Test the index with a sample query\n",
    "test_user_embedding = user_embeddings[0:1].astype('float32')\n",
    "distances, indices = index.search(test_user_embedding, k=5)\n",
    "\n",
    "print(f\"\\nTest search for user 0:\")\n",
    "for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "    job_title = jobs_clean.iloc[idx]['job_title']\n",
    "    print(f\"  {i+1}. {job_title} (distance: {dist:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d491684",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b882e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model - using JobBERT-v3 for specialized job embeddings\n",
    "print(\"Setting up embedding model...\")\n",
    "\n",
    "# Try JobBERT-v3 first (specialized for job titles), fallback to all-MiniLM-L6-v2\n",
    "try:\n",
    "    model_name = \"TechWolf/JobBERT-v3\"  # Specialized job title embedding model\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    print(f\"‚úÖ Successfully loaded JobBERT-v3 (specialized for job embeddings)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è JobBERT-v3 not available: {e}\")\n",
    "    print(\"Falling back to all-MiniLM-L6-v2...\")\n",
    "    model_name = \"all-MiniLM-L6-v2\"  # Fast and efficient fallback model\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    print(f\"‚úÖ Loaded fallback model: {model_name}\")\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"\\nGenerating user embeddings...\")\n",
    "user_embeddings = embedding_model.encode(\n",
    "    users_clean['user_text'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(\"Generating job embeddings...\")\n",
    "job_embeddings = embedding_model.encode(\n",
    "    jobs_clean['job_text'].tolist(), \n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Generated embeddings:\")\n",
    "print(f\"  ‚Ä¢ User embeddings shape: {user_embeddings.shape}\")\n",
    "print(f\"  ‚Ä¢ Job embeddings shape: {job_embeddings.shape}\")\n",
    "print(f\"  ‚Ä¢ Model used: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39e2f5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **Data Loading**: Successfully loaded user profiles and job catalog  \n",
    "‚úÖ **Data Cleaning**: Cleaned text fields, parsed skills and interests  \n",
    "‚úÖ **Feature Engineering**: Created user-job compatibility features  \n",
    "‚úÖ **Embeddings**: Generated semantic embeddings using sentence-transformers  \n",
    "‚úÖ **Vector Database**: Set up FAISS index for efficient similarity search  \n",
    "‚úÖ **Data Export**: Saved all processed data for training pipeline\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run `02_train_xgboost.ipynb` to train the reranking model\n",
    "2. Use `03_evaluate.ipynb` to measure system performance  \n",
    "3. Try `04_inference_demo.ipynb` for interactive recommendations\n",
    "\n",
    "**Key Outputs:**\n",
    "- Processed user profiles with embeddings\n",
    "- Job catalog with semantic vectors\n",
    "- FAISS index for fast similarity search\n",
    "- Training features for reranking model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
