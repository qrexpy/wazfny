{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf035bf",
   "metadata": {},
   "source": [
    "# Career Recommender System - XGBoost Reranker Training\n",
    "\n",
    "This notebook trains an XGBoost model to rerank job recommendations based on structured features derived from user profiles and job requirements.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load preprocessed data and features  \n",
    "2. Prepare training data with positive/negative examples\n",
    "3. Train XGBoost reranker with hyperparameter tuning\n",
    "4. Evaluate model performance on validation set\n",
    "5. Save trained model for inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882bd2b",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Run this cell first to install all required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Colab/Kaggle environments\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn plotly\n",
    "!pip install sentence-transformers transformers torch\n",
    "!pip install faiss-cpu xgboost\n",
    "!pip install python-jobspy>=1.1.79 datasets>=2.14.0 serpapi>=1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fb0d6",
   "metadata": {},
   "source": [
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9de70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0965e",
   "metadata": {},
   "source": [
    "## 3. Load Preprocessed Data\n",
    "\n",
    "If you haven't run the preprocessing notebook yet, we'll create sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load preprocessed data, create sample if not available\n",
    "try:\n",
    "    # Load from preprocessing notebook\n",
    "    users_df = pd.read_pickle('models/users_processed.pkl')\n",
    "    jobs_df = pd.read_pickle('models/jobs_processed.pkl')\n",
    "    features_df = pd.read_pickle('models/training_features.pkl')\n",
    "    print(\"‚úÖ Loaded preprocessed data from models/\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Preprocessed data not found. Creating sample training data...\")\n",
    "    \n",
    "    # Create sample training features for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 500\n",
    "    \n",
    "    # Generate sample features\n",
    "    features_data = {\n",
    "        'user_id': np.random.randint(1, 21, n_samples),\n",
    "        'job_id': np.random.randint(1, 31, n_samples),\n",
    "        'gpa_normalized': np.random.normal(0.75, 0.15, n_samples).clip(0, 1),\n",
    "        'experience_years': np.random.randint(0, 10, n_samples),\n",
    "        'education_level_match': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),\n",
    "        'experience_match': np.random.choice([0, 1], n_samples, p=[0.4, 0.6]),\n",
    "        'skill_overlap': np.random.beta(2, 5, n_samples),  # Skewed towards lower values\n",
    "        'education_overqualified': np.random.choice([0, 1, 2], n_samples, p=[0.6, 0.3, 0.1]),\n",
    "        'experience_overqualified': np.random.randint(0, 5, n_samples),\n",
    "        'salary_avg_normalized': np.random.beta(3, 2, n_samples),\n",
    "        'location_match': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "    }\n",
    "    \n",
    "    features_df = pd.DataFrame(features_data)\n",
    "    \n",
    "    # Create realistic labels based on features\n",
    "    label_prob = (\n",
    "        0.3 * features_df['education_level_match'] +\n",
    "        0.2 * features_df['experience_match'] +\n",
    "        0.3 * features_df['skill_overlap'] +\n",
    "        0.1 * features_df['gpa_normalized'] +\n",
    "        0.1 * features_df['location_match']\n",
    "    )\n",
    "    \n",
    "    features_df['label'] = np.random.binomial(1, label_prob.clip(0, 1))\n",
    "    \n",
    "    print(\"‚úÖ Created sample training data\")\n",
    "\n",
    "print(f\"Training data shape: {features_df.shape}\")\n",
    "print(f\"Positive examples: {features_df['label'].sum()}\")\n",
    "print(f\"Negative examples: {len(features_df) - features_df['label'].sum()}\")\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a3e3b",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "feature_cols = ['gpa_normalized', 'skill_overlap', 'experience_years', \n",
    "                'education_level_match', 'experience_match', 'location_match']\n",
    "\n",
    "for i, col in enumerate(feature_cols):\n",
    "    if col in ['education_level_match', 'experience_match', 'location_match']:\n",
    "        # Bar plot for binary features\n",
    "        feature_counts = features_df[col].value_counts()\n",
    "        axes[i].bar(feature_counts.index, feature_counts.values, alpha=0.7)\n",
    "        axes[i].set_title(f'{col.replace(\"_\", \" \").title()}')\n",
    "    else:\n",
    "        # Histogram for continuous features\n",
    "        axes[i].hist(features_df[col], bins=20, alpha=0.7)\n",
    "        axes[i].set_title(f'{col.replace(\"_\", \" \").title()}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "print(\"Feature Correlations:\")\n",
    "feature_columns = [col for col in features_df.columns if col not in ['user_id', 'job_id', 'label']]\n",
    "corr_matrix = features_df[feature_columns + ['label']].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf3485",
   "metadata": {},
   "source": [
    "## 5. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "feature_columns = [col for col in features_df.columns if col not in ['user_id', 'job_id', 'label']]\n",
    "X = features_df[feature_columns].values\n",
    "y = features_df['label'].values\n",
    "\n",
    "# Handle any missing values\n",
    "X = np.nan_to_num(X, nan=0.0)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Feature columns: {feature_columns}\")\n",
    "\n",
    "# Split into train/validation/test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"Train: {X_train.shape[0]} samples ({y_train.sum()} positive)\")\n",
    "print(f\"Validation: {X_val.shape[0]} samples ({y_val.sum()} positive)\")\n",
    "print(f\"Test: {X_test.shape[0]} samples ({y_test.sum()} positive)\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef60ce",
   "metadata": {},
   "source": [
    "## 6. Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XGBoost model with initial parameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training XGBoost reranker...\")\n",
    "xgb_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_val_scaled, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model training completed!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nüìä Model Performance:\")\n",
    "print(f\"AUC: {auc_score:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e471e2",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187cbefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"üîç Feature Importance Rankings:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_df, x='importance', y='feature', palette='viridis')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29047aa9",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d66c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV (uncomment to run)\n",
    "# Warning: This can take several minutes to complete\n",
    "\n",
    "run_tuning = False  # Set to True to run hyperparameter tuning\n",
    "\n",
    "if run_tuning:\n",
    "    print(\"üîß Starting hyperparameter tuning...\")\n",
    "    \n",
    "    param_grid = {\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    xgb_tuned = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        xgb_tuned, param_grid,\n",
    "        cv=3,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_tuned = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc_tuned = roc_auc_score(y_test, y_pred_tuned)\n",
    "    print(f\"Tuned model AUC: {auc_tuned:.4f}\")\n",
    "    \n",
    "    # Update model if better\n",
    "    if auc_tuned > auc_score:\n",
    "        xgb_model = best_model\n",
    "        print(\"‚úÖ Using tuned model (better performance)\")\n",
    "    else:\n",
    "        print(\"‚úÖ Keeping original model (tuning didn't improve)\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping hyperparameter tuning (set run_tuning=True to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f13aa",
   "metadata": {},
   "source": [
    "## 9. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and preprocessing components\n",
    "model_data = {\n",
    "    'model': xgb_model,\n",
    "    'scaler': scaler,\n",
    "    'feature_columns': feature_columns,\n",
    "    'metrics': {\n",
    "        'auc': auc_score,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save model\n",
    "with open('models/reranker_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_type': 'XGBClassifier',\n",
    "    'feature_columns': feature_columns,\n",
    "    'training_samples': len(X_train),\n",
    "    'validation_auc': float(auc_score),\n",
    "    'feature_importance': {feat: float(imp) for feat, imp in zip(feature_columns, feature_importance)}\n",
    "}\n",
    "\n",
    "with open('models/reranker_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "print(f\"üìÅ Files saved:\")\n",
    "print(\"  - models/reranker_model.pkl\")\n",
    "print(\"  - models/reranker_metadata.json\")\n",
    "\n",
    "# Test loading the model\n",
    "print(\"\\nüß™ Testing model loading...\")\n",
    "with open('models/reranker_model.pkl', 'rb') as f:\n",
    "    loaded_model_data = pickle.load(f)\n",
    "\n",
    "test_input = X_test_scaled[:5]  # Test with 5 samples\n",
    "test_predictions = loaded_model_data['model'].predict_proba(test_input)[:, 1]\n",
    "print(f\"‚úÖ Model loading test successful!\")\n",
    "print(f\"Sample predictions: {test_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5470c0ee",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **XGBoost Reranker Training Complete!**\n",
    "\n",
    "**What we accomplished:**\n",
    "- Loaded/created training data with user-job compatibility features\n",
    "- Trained XGBoost classifier for job recommendation reranking\n",
    "- Evaluated model performance with AUC, precision, recall metrics\n",
    "- Analyzed feature importance to understand key factors\n",
    "- Saved trained model for use in recommendation pipeline\n",
    "\n",
    "**Key Results:**\n",
    "- Model learns to predict job relevance based on structured features\n",
    "- Most important features typically include skill overlap and education matching\n",
    "- Model can be used to rerank semantic search results for better recommendations\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run the evaluation notebook to measure ranking metrics (NDCG@k)\n",
    "2. Use the inference demo notebook to test recommendations\n",
    "3. Integrate into production recommendation system"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
